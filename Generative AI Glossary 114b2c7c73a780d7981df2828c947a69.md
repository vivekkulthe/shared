# Generative AI: Glossary

### A

| **Word** | **Description/Definition.** |
| --- | --- |
| Adaptive Fine Tuning | Continuously updating and re-training a model with new data to enhance its accuracy and performance |
| Adversarial Training | A process in which two neural networks engage in a competition to enhance their respective performance |
| Algorithm | A set of instructions or rules used by a Machine Learning model to make predictions |
| Architecture | The structure and design of neural networks or AI models |
| Artificial Intelligence (AI) | The field of designing and creating intelligent machines or computer programs |
| Association Rule Mining | An unsupervised learning technique used to discover new rules to describe the patterns within a dataset |
| Attributes | Distinctive characteristics or features of data which are used to specify the desired characteristics of the output |
| Autoregressive Model | A generative model that predicts the next element in a series based on the pattern of the previous elements |

### B

| **Word** | **Description/Definition** |
| --- | --- |
| Balanced Sampling | A sampling technique that addresses concerns of data imbalance within the training data |
| Back-propagation | A fundamental technique used in the training of AI models, whereby the model learns from errors and adjusts certain parameters to improve its performance and reduce further errors |
| Baseline Model | A reference model used to compare performance of the modified or tuned model to assess changes |
| Batch Size | The quantity of training examples that are processed simultaneously before the model parameters are updated |
| Bi-directional Encoder Representations from Transformers (BERT) | A transformer model used for encoding languages and generating content |
| Bias | Skewed characteristics within the training dataset, where some classes are over-represented, resulting in the model displaying a similar bias towards those classes |

### C

| **Word** | **Description/Definition** |
| --- | --- |
| Class Imbalance | A situation where some classes in the training dataset have significantly fewer examples than others |
| Classification Algorithm | A Machine Learning algorithm that categorises data into various classes |
| Clustering | An unsupervised learning method to identify inherent clusters or classes present within a dataset |
| Code Snippets | Compact sections of code designed for specific tasks or purposes |
| Coherence | Logical and consistent nature of generated output from AI models |
| Computational Expense | The power required for training an AI model, usually measured in terms of time and resources, along with software and hardware requirements |
| Conditional GANs | Generational Adversarial Networks (GANs) that can generate samples based on specific attributes or conditions provided as input |
| Control Factors | Parameters or variables that affect the output of an AI model |
| Convergence | The point at which an AI model's loss function stabilises during the training process |
| Convolutional | Refers to filter-based operations used to capture patterns in data |
| Convolutional Neural Network (CNN) | AI model used to process grid-like data in a sequential format |
| Curriculum Learning | A technique that gradually increases the complexity of the training data to build a strong foundation for the AI model |

### D

| **Word** | **Description/Definition** |
| --- | --- |
| Data Augmentation | The creation of synthetic data to expand the training dataset and improve an AI model's performance |
| Data Efficiency | The ability of an AI model to perform with limited training data |
| Data Imbalance | A scenario where certain classes are under-represented within a training dataset |
| Deep Convolutional GANs (DCGANs) | A type of GAN AI model that uses convolutional neural networks for the generator and discriminator components |
| Deep Learning | Subset of Machine Learning that focuses on training neural networks with multiple layers deep within the model |
| Deepfake | Manipulated multimedia content that is often created for malicious purposes |
| Discriminative Model | An ML model that is trained using the probability distribution of only one variable of a function |
| Discriminator | The component of a GAN model that acts as a binary classifier to distinguish between real and fake data |
| Distribution | The pattern or spread of values of data within a dataset, which can also be represented visually on a graph |
| Diversity | The range or variety of different elements within the data generated by an AI model |
| Dropout | A regularisation technique whereby a certain number of neurons within the neural network are deactivated to prevent overfitting of the AI model |

### E

| **Word** | **Description/Definition** |
| --- | --- |
| Experimental Iteration | A cyclical process of training and evaluation an AI model with different settings to improve its performance |
| Exploratory Data Analysis (EDA) | The process of visualising and analysing data in order to better understand its characteristics and quality |
| Evaluation Metrics | A collection of metrics used to estimate the performance of aÂ  machine learning model |

### F

| **Word** | **Description/Definition** |
| --- | --- |
| Fair Learning | The practice of ensuring that an AI model is trained in an unbiased manner despite imbalances in the training dataset |
| Feature Extraction | The process of analysing data to identify its significant characteristics |
| Feature Matching | A technique used to align the features of generated data samples with the features of real data samples |
| Feature Selection | The process of selecting specific data characteristics or attributes for training AI models |
| Fine Tuning | The process of refining pre-trained AI models by adjusting its parameters for better performance on specific tasks |
| Flow-based Model | An AI model that learns the transformation of simple to complex data distribution and then generates samples by applying the inverse transformation |
| Frechet Inception Distance | A metric that is used to measure the similarity between the data distribution of generated samples and real samples |
| Fully-connected Layer | A neural network layer that connects every neuron to those in adjacent layers, allowing the AI model to use complex feature combinations for decision-making |

### G

| **Word** | **Description/Definition** |
| --- | --- |
| Generative Adversarial Network (GAN) | A Machine Learning algorithm that uses two neural networks competing with each other to improve the performance of both networks |
| Generator | A component of GAN models that generates data samples resembling the distribution of the training dataset |
| Github | Provider of Internet hosting for software development and version control using Git |
| Gradient | A parameter that guides an AI model on how much to adjust its parameters to reduce errors in prediction by selecting the configuration that results in the highest improvement |
| Gradient Penalty | A method used to control the gradients of neural networks in GAN models for more stability during training |

### H

| **Word** | **Description/Definition** |
| --- | --- |
| Hyperparameter | A pre-defined parameter whose value is controlled to help guide the ML model training process and optimise performance. Hyperparameters are not updated by the model itself during the training process. |

### I

| **Word** | **Description/Definition** |
| --- | --- |
| Inception Score | A quantitative metric that represents a combination of the diversity and the quality of generated data samples |
| Initialising Weights | The process of pre-defining parameters before training an AI model |
| Input Sequences | Initial prompts given to an AI model that influence the output generated |
| Isolated Single-task Learning | The process of training an AI model to perform a single task independently from other related tasks |

### J

| **Word** | **Description/Definition** |
| --- | --- |
| Joint Training | The process of simultaneously training the generative and discriminative components of a GAN model to improve its performance |
| JSON | JavaScript Object Notation. A format for storing and transmitting data |

### L

| **Word** | **Description/Definition** |
| --- | --- |
| Large Language Model (LLM) | AI models trained with billions or even trillions of parameters specifically for content generation tasks |
| Latent Space | A compressed representation of the input data within a generative AI model |
| Layer | A layer within a neural network where neurons are organised |
| Learning Rate | A hyperparameter that determines the step size of parameter updates during training |
| Linear Regression | An algorithm that models a linear relationship between a continuous target variable and one or several continuous features |
| Logistic Regression | An algorithm that uses a logistic function on the input data to predict the probability of data belonging to a particular class |
| Loss Function | A mathematical function that measures the difference between the predicted out and the actual generated output |
| Long Short-Term Memory (LSTM) | A type of Recurrent Neural Network model that addresses long-term dependencies within data using memory cells |

### M

| **Word** | **Description/Definition** |
| --- | --- |
| Machine Learning | A subset of Artificial Intelligence, ML enables systems to learn and improve through experience without explicit programming |
| Majority Class | A class within the dataset that has more examples than the rest |
| Mini-batch Discrimination | A training technique that enhances the stability and diversity of generated data samples during model training |
| Mode Collapse | A situation encountered while training GANs wherein the network fails to capture or learn the entire diversity in the training dataset, resulting in the model producing limited or repetitive output |
| Model | Algorithms that process data and learn from it, including deep learning and neural networks |
| Multi-layer Perceptron | A basic form of deep neural networks wherein information flows in one direction only, thus rendering the model incapable of adjusting its own parameters through back-propagation |
| Multi-modal AI | An AI model that is capable of generating content across different media types, such as text, images, audio and video |

### N

| **Word** | **Description/Definition** |
| --- | --- |
| Nash Equilibrium | A point where the samples generated by the Generator component of a GAN model match real data and cannot be differentiated by the Discriminator component |
| Natural Language Processing (NLP) | A field of AI that enables computers to understand and generate content in human languages |
| Neural Network | A computational model inspired by the human brain, consisting of multiple interconnected nodes (neurons) arranged in layers to facilitate complex processing and decision-making |
| Neuron | Interconnected nodes within a neural network that process input data and produce output |
| Noise | Data that is incorrect and interferes with the analysis of accurate data within a dataset |
| Noise Addition | The technique of introducing random noise into input data to augment the dataset and improve a model's performance |

### O

| **Word** | **Description/Definition** |
| --- | --- |
| Optimisation Algorithm | Specific algorithms used to update a model's parameters during training |
| Outlier | A data point within a dataset that is significantly different from the average characteristics of the others |
| Overfitting | A situation where an ML model gives high accuracy with a training dataset, but low accuracy with a new dataset |
| Oversampling | A technique involving repeating samples from minority classes to balance datasets and improve model performance |

### P

| **Word** | **Description/Definition** |
| --- | --- |
| Parameter | A variable that is controlled to define a system or operation |
| Persona | A fictional character that can be assigned to a GAN model through an input prompt for additional context |
| Pooling Layer | A layer within a neural network that focuses on summarising specific regions of the training dataset to help the model to focus on important features, thus improving performance |
| Pre-trained Model | An AI model that has been trained on a related task and can be used as a starting point for training a model on a new task |
| Prompt Engineering | The technique of crafting clear and precise instructions to AI models to help guide and refine its output |

### R

| **Word** | **Description/Definition** |
| --- | --- |
| Recurrent Neural Network (RNN) | A neural network model that uses looped connection to process sequential or time series data |
| Reducing Dimensions | Techniques used during training of AI models to reduce the number of significant features or attributes within a dataset |
| Regression Algorithm | A Machine Learning algorithm that is used to predict data that is in the form of continuous numerical values |
| Regularisation | Techniques used to prevent overfitting of an AI model and to manage model complexity |
| Relevance Enhancement | The process of incorporating new data into a model to improve its accuracy |
| Replicability | A metric that measures whether a model can generate consistent output using the same input data |

### S

| **Word** | **Description/Definition** |
| --- | --- |
| Sample | A dataset or part of a dataset that is used as information or input for AI operations |
| Sample Stability | A process to ensure that the samples generated by an AI model are consistent and do not vary drastically |
| Scaling | Adjustment of data values for compatibility with a particular AI model |
| Self-supervised Learning | A training technique whereby an AI model generates its own signals to adjust parameters based on the input data |
| Similarity | A measure of closeness between generated data and real data |
| SMOTE (Synthetic Minority Oversampling Technique) | A technique used during training of AI models that involves balancing class distribution within a dataset by oversampling data from minority classes |
| SoftMax Activation | A function used in probabilistic AI models to calculate probabilities |
| Specificity | A metric that measures the level of detail and precision in the instruction or context given to an AI model |
| Supervised Learning | A technique where the ML model is given labelled data as input and output for training it to predict the label of unlabelled data. |
| Support Vector Machines (SVM) | Discriminative AI models that are used for classification tasks |
| Syntax | The rules that define the structure of a language |
| Synthetic Data | Artificially generated data that is used for training AI models |

### T

| **Word** | **Description/Definition** |
| --- | --- |
| Temperature Parameter | A hyperparameter that controls the level of randomness in the output generated by an AI model |
| Tokenisation | Process of breaking text into smaller components or tokens for further analysis or processing |
| Transformation | The process of changing the format, structure or values of data |
| Transformers | AI models that facilitate the training of larger models by eliminating the need to use labelled data |

### U

| **Word** | **Description/Definition** |
| --- | --- |
| Unsupervised Learning | A technique where an AI model is provided with unlabelled data for training it to identify labels as the output |
| Underfitting | A situation where an AI model gives low accuracy with both, the training dataset and a new dataset |
| Undersampling | A technique to balance uneven datasets by keeping all of the data in the minority class and decreasing the size of the majority class |

### V

| **Word** | **Description/Definition** |
| --- | --- |
| Variational AutoEncoder (VAE) | An AI model that learns to patterns from compressed representations of the input dataset |
| Validation Data | A dataset that is used to evaluate a model's performance during training |
| Variance | The difference between the actual value and the expected value of data that an AI model is trying to predict |
| Vanishing Gradients | A situation during training of AI models wherein the gradient becomes very small, resulting in progressively slower learning |

### W

| **Word** | **Description/Definition** |
| --- | --- |
| Wasserstein Distance | A metric used to measure the dissimilarity between two probability distributions |
| Wasserstein GANs (WGANs) | AI models that use Wasserstein Distance to measure the difference between the generated output and real data distributions |
| Weight Clipping | A regularisation technique that limits the values of the model's weights during training |
| Weighting | Adjusting the importance of a component of a dataset within a loss function of an AI model |